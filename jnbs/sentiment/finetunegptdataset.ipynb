{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "gpt_df = pd.read_csv(\"/kaggle/input/gpt-dataset/gpt_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "PAT = user_secrets.get_secret(\"pat\")\n",
    "\n",
    "\n",
    "GITHUB_USERNAME = \"vladkisin\"\n",
    "REPO_NAME = \"workmind-dev\"\n",
    "REPO_URL = f\"https://{GITHUB_USERNAME}:{PAT}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "os.system(f\"git clone {REPO_URL}\")\n",
    "os.chdir(\"/kaggle/working/workmind-dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=user_secrets.get_secret(\"wandb_pat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from adapters import AutoAdapterModel, AdapterTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from experiment.wandb.sentiment import SentimentExperiment\n",
    "from tuners.adapter import AdapterFineTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare numeric labels\n",
    "map2label = { 0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "gpt_df['text'] = gpt_df['text'].astype(str)\n",
    "gpt_df['label'] = gpt_df['sentiment_label'].map({v: k for k,v in map2label.items()})\n",
    "\n",
    "def preprocess_data(batch, tokenizer):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=min(tokenizer.model_max_length, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## PARAMETRIZE HERE ##\n",
    "model_name = \"roberta-base\" #\"microsoft/deberta-v3-large\"\n",
    "experiment_prefix = f\"5-fold lora adapter for {model_name}\"\n",
    "experiment_suffix = \" ChatGPT-o1 Generated Data\"\n",
    "PROJECT_NAME = \"workmind-email-data\"\n",
    "## PARAMETRIZE HERE ##\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "map_func = functools.partial(preprocess_data, tokenizer=tokenizer)\n",
    "\n",
    "data = gpt_df[['text', 'label', 'user_id']].to_dict(orient=\"list\")\n",
    "y_gpt_email = gpt_df[\"sentiment_label\"].tolist()\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "texts = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "with SentimentExperiment(\n",
    "    None, # No analyzer provided since we collect predictions in 5 folds\n",
    "    experiment_prefix + experiment_suffix, \n",
    "    y_gpt_email,\n",
    "    project_name=PROJECT_NAME\n",
    ") as experiment:\n",
    "    # Placeholder for fold predictions\n",
    "    all_predictions = [None] * len(dataset)  # Placeholder to maintain original order\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # K-Fold Cross-Validation\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(texts)):\n",
    "        print(f\"Processing Fold {fold + 1}...\")\n",
    "\n",
    "        train_texts, test_texts = np.array(texts)[train_idx], np.array(texts)[test_idx]\n",
    "        train_labels, test_labels = np.array(labels)[train_idx], np.array(labels)[test_idx]\n",
    "        # Create train and test datasets\n",
    "        train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "        test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
    "        train_dataset = train_dataset.map(map_func, batched=True)\n",
    "        test_dataset = test_dataset.map(map_func, batched=True)\n",
    "\n",
    "        adapter_tuner = AdapterFineTuner(\n",
    "            model_name_or_path=model_name,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            adapter_name=\"sentiment-head\",\n",
    "            num_labels=3,\n",
    "            id2label=map2label,\n",
    "            learning_rate=5e-4,\n",
    "            num_train_epochs=2,\n",
    "            train_batch_size=16\n",
    "        )\n",
    "        adapter_tuner.prepare_model()\n",
    "        adapter_tuner.train(trainer_class=AdapterTrainer)\n",
    "\n",
    "\n",
    "        predictions = adapter_tuner.trainer.predict(test_dataset)\n",
    "        preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "        for idx, pred in zip(test_idx, preds):\n",
    "            all_predictions[idx] = pred  # Assign prediction to the original order\n",
    "\n",
    "    experiment.log_metrics([map2label[x] for x in dataset['label']], \n",
    "                           [map2label[x] for x in all_predictions], \n",
    "                           user_ids=dataset['user_id'])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6505548,
     "sourceId": 10696424,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}